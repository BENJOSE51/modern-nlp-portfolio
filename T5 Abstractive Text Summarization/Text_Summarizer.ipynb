{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucbNdarr5_3j"
   },
   "source": [
    "# LLM Text Summarizer ‚Äî Fine-Tuning T5-small on CNN/DailyMail\n",
    "\n",
    "**Goal:** Fine-tune a pre-trained T5-small model to perform abstractive text summarization, evaluating performance using the ROUGE metric.\n",
    "\n",
    "**Environment:** Google Colab\n",
    "\n",
    "---\n",
    "## 1.1 Install Dependencies & Set Global Variables\n",
    "The following packages are essential for the NLP pipeline: `transformers` for the model, `datasets` for data handling, and `evaluate` for metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8GTTqUa96FDK"
   },
   "outputs": [],
   "source": [
    "# Install packages\n",
    "!pip install -U transformers datasets evaluate accelerate sentencepiece rouge-score nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y2JTa8R96I7h"
   },
   "outputs": [],
   "source": [
    "#Setup and Reproducibility\n",
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Re5iWQm61u4"
   },
   "outputs": [],
   "source": [
    "# Global Project Setup\n",
    "PROJECT_DIR = Path('/content/text_summarizer_project_final')\n",
    "PROJECT_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dbVa-2Wc6-dx",
    "outputId": "74285d43-0d04-407e-ddf2-aa78c0de1528"
   },
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BWGNVa857Axv",
    "outputId": "eb698ae7-211f-444e-a266-ce82020bc5e4"
   },
   "outputs": [],
   "source": [
    "# Data sizes\n",
    "TRAIN_SIZE = 500\n",
    "VAL_SIZE = 200\n",
    "\n",
    "print('Project dir:', PROJECT_DIR)\n",
    "print('TRAIN_SIZE=', TRAIN_SIZE, 'VAL_SIZE=', VAL_SIZE)\n",
    "print('Using GPU:', torch.cuda.is_available(), 'Device count:', torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z6-dODH-97Ud"
   },
   "source": [
    "# Data Preparation and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMKnLMQz-C0V"
   },
   "source": [
    "## Load and Subset the Dataset\n",
    "We use the **CNN/DailyMail** dataset, a gold-standard benchmark for summarization, which contains news articles and corresponding professional human-written summaries (highlights). We select a small subset for rapid fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 481,
     "referenced_widgets": [
      "cb49cc10386e4a2c85d2eb81f9198555",
      "78d9a059b09948a88965677a33cdb6fc",
      "541dd8da4af546cf984279f331024e73",
      "22afe65b9d974657ab1408bc13bcf44f",
      "29649741e4fa4adfa2e263e35cd9645b",
      "5391526d4f864922b71ab3e1e4e12c63",
      "3cb3c8b1247a4441a513fd0ec70ce1f5",
      "67f1ce51dbdb4977b61c8910fa52fd0b",
      "1e98e9f0a62a44a7b77f2cdd1ba2f2b9",
      "13ed8e7cfb85468dbbad52f3589b2b5a",
      "b9c45148b64c4e0f9b50b60a8e42a588",
      "92056b1d45444abaa1518bb72af5cfcd",
      "2bd2e7fe607243a6a3735246b9204b7a",
      "96d908e187204923beb2dd357c0fb610",
      "9525d6b13f144f91a71cba76cc6e189d",
      "4e1f491699fc4d7599731ddc8c77c00f",
      "ce2fcee79a904f6985d074c7cc66a336",
      "a727ae62d9944825a041a4a1bb70b3e1",
      "0604892412f84396867697e984a7c4da",
      "05e21a5c84f042afa1681868602a76e1",
      "5b27bb8589a149119378954f368927a8",
      "22c6f107053947f28392d77cc88a4118",
      "92660f040d8e420ebe0f45d2a23c2089",
      "ddb2e8f340d74697a87baa678df365ef",
      "e736e4bb59a44951bc3f8adf26324519",
      "62b7e0012abf455eb6ccae5b51f82541",
      "a712917dd526422699991cf25f10b605",
      "e2a09fd88ba34012938631b60ce64ad5",
      "15ed7f725800458dab34fc34e3689f86",
      "cc8ef3a10c2547cb954ad18708fb60ff",
      "eb13a185a1c942a1ba8d57dca3c8bf86",
      "40b48756ab084c59ba8602798c4add52",
      "f35e5c80159943bd94f65dba9d6583c3",
      "451d7b901028456ea4702ece24691afd",
      "36faa95835f64a368dc82e2b0c3ae787",
      "562d34c114a44d53b15d765988fd7fdf",
      "e7bbd2eb065c4199b8b36eb3f108b5f8",
      "220dbf0f03c34366aaa7376d008fd486",
      "70c2fc969e0b481dbdd9dfb9d10ea36b",
      "7e46b1b6b62647e982e93f6146e6a7c7",
      "7dd052f47143486bb868c98f70eb3352",
      "029ec7cdcf75465abc1a1855e5837f90",
      "31014874728f487bafbc2b57a14d8bdb",
      "3ccebeaa9293476ebf98851658276071",
      "395e1dabf1854fd8a7bf66bd5432c7c6",
      "034b34d8314c4b0f9623107747325e00",
      "134c92b456f948129f70adaeee15f6e1",
      "af88665442db45cb9c7e13e363a171d8",
      "61f6f54473a24027941299065d090121",
      "4494753252074ab69845cbde55900a6a",
      "68eb960651114963bac617da8a263117",
      "ffa8a7ebe9104b5abeff986d550d33a0",
      "0241c29bffc24494b362c050760f59c2",
      "fb64cdadf6ba4438a18c31c73ef2e55f",
      "d837c7bc31f04082a77a3760a0d96f5b",
      "2a5c3214531c432189ea9e446d7b3736",
      "0d9559f9f2254b6898aa4e06e0afd233",
      "5a6f281b719f40588a55bea1a59d21c0",
      "e245b09d6f7f4be6a198e385f5a63695",
      "8cfa11f912134ae4947a08976504f6a7",
      "5883b34ea5d8439a9ac93623f22c8cbb",
      "3eff5d788eee47dc8755f8cd6a1b3fc6",
      "6af7bf7de34b41d0903aa01d2bda3662",
      "56f67f9f3dca4a35ba81a306a2798d87",
      "2f2d39ef686846448c594543824b2b62",
      "b288bf76665f48c7bcef9db3d3631225",
      "932313ebadae4dde8f0cbcd3e893e250",
      "82e1ba45bc6f4fc9bbf01d08207cfa19",
      "f9cbf9fdbfd046019e5a77e733ef41a1",
      "54571625c7c347e086670fd3956c45d2",
      "dc7dceca1f774c44bfa640d4985f9f0b",
      "de1ea9f7975749edb2f9f9f24b63951a",
      "5fc41fdcabac473eb0b32dd439f3bbe5",
      "ccd9f40201f748399c7251493c25b5cc",
      "1841c78eb26a46c19f5196f03d014dd3",
      "4b3de555cbe749cb9e7b9739433d3aa8",
      "d8abbd1ea0b64a04bbb37384ea12626b",
      "fc9c581ed4c14021b83b95b427ad28f7",
      "13c3bd1db91e431db14d4887daa9bb33",
      "d60b22f2f01a4d5489b7371e477ce5da",
      "b0b00b651255422ead1a0666f2a86dbb",
      "a352126f8c544643a2ab9e42bcf0f515",
      "ec44adc35b1b4e80ae334a3913812af5",
      "65a0a313cb844e5b862c36471d986e77",
      "dcb98a6dd4344711a49a1e62b2c295fe",
      "c941123a44f743b9854badffda895671",
      "c27c6c4f4e33449bb22228752134b940",
      "af286a1ed1c441fb86b239348aabedd4",
      "fe0957c41df64bf7b0231a9160e13bff",
      "799f1916863f4812ac9586d046101412",
      "b72e45cf0c5b44da8ee5cd382fc98801",
      "e781b20ba9854caea611c3df2150c1f2",
      "cdfb6798dc1f46f7b75da8fefb0a0d18",
      "5b06012389fc498daeb8255e86d66689",
      "ed37360c790f47bab52a95780d6a4861",
      "e2edbf49d1b94f1289e74c2132059747",
      "ac9e4b6d9ade46d8bcd692d760f3147e",
      "cfa029ce234d43a286374ff575b8552c",
      "5f8436dde9ef4de58cc2dfc1a526e20e"
     ]
    },
    "id": "mDgzXbQM7G4M",
    "outputId": "b1b9bd5d-6206-421a-a965-8daf3eff1f79"
   },
   "outputs": [],
   "source": [
    "#Download and Prepare Dataset\n",
    "print('Downloading CNN/DailyMail (may take a few minutes)...')\n",
    "raw = load_dataset('cnn_dailymail', '3.0.0')\n",
    "train_ds = raw['train'].select(range(TRAIN_SIZE))\n",
    "val_ds = raw['validation'].select(range(VAL_SIZE))\n",
    "print('Loaded subsets: train=', len(train_ds), 'val=', len(val_ds))\n",
    "print('Sample keys:', train_ds.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_BirYS7j-Je7"
   },
   "source": [
    "## Tokenization Strategy\n",
    "We load the `t5-small` tokenizer. T5 is a Sequence-to-Sequence model that requires a task prefix, **`summarize:`**, to be prepended to the input text. This conditions the model to perform the intended task. We also set limits to prevent excessive GPU memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 130,
     "referenced_widgets": [
      "504b7f2e54ae4e95bcedf26a403b8366",
      "321319f29a0f4ba8ad62f9b0d11269e3",
      "c716f737bfc245ddbb09f0d128f3d127",
      "51cb12cb4a8140d1b1dc4cb86f32d1ac",
      "4b721366c98146af8ee25d53a2d49d34",
      "75e08ea761a74a6c8ea15f8c096b9b04",
      "92ee55009f88471e9506f3c7f07b3e8f",
      "5665c78a925041e3abee999f93cd727e",
      "09781e4540b940cb8ad93e6084793602",
      "71f553492fb24c2f895a5f8114577b01",
      "4e401b10c4aa4fa9a8f58a1fddbf222d",
      "3592d5024621443fa622434e381c6c6f",
      "6aa9cab81c9d4fc7b8345552c6061219",
      "429d0d973b974c91a832edb75a96eabf",
      "b908cd49d2f541119c51561e020b26ab",
      "a8551b5faac1460e82cfd1ce68236f98",
      "cc405d386040464ebf441422a04ca123",
      "c792ccd5c0d347b4a7052cbc55146c24",
      "0cf7a7afdb384a079964aa5ca8731a79",
      "c966f7c1d0a34c3e9d75a9d6abaaabda",
      "bb8de337223f41818588cf5f7c7839e2",
      "6a959772fc8f4e09ac9145f6a6b8f706",
      "e9756e15138c4eb49c161ec550b92864",
      "c197c48697e8479b9e5c679ecf02ee47",
      "5fd89e7b7a66463981fd19e363222d3f",
      "8c07d6b26dae41a887b78c2a2c78d5e5",
      "8521acf4d5c84052aef859d69579e37d",
      "f37ede48a6af40389391f58a25698dac",
      "477e62abce7f4a62bf350dda5920843b",
      "5e8d554bbda6449989a01f4b84e504ee",
      "1ccca831994041b59ad3252e5af11b2d",
      "fe21922f2c8147a584980737a6b0b620",
      "e0f823ee0c9248068cb24259ae9cb89c"
     ]
    },
    "id": "GihGdUEs7Q3S",
    "outputId": "f717f9dc-f029-475d-8337-6a718b1464ec"
   },
   "outputs": [],
   "source": [
    "#Tokenizer setup\n",
    "MODEL_NAME = 't5-small'\n",
    "\n",
    "print('Loading tokenizer:', MODEL_NAME)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "71J0YJ527bjp"
   },
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def preprocess_function(examples, max_input_length=512, max_target_length=128):\n",
    "    inputs = examples['article']\n",
    "    # Add prefix for T5 models (e.g., 'summarize: ')\n",
    "    inputs = [f\"summarize: {text}\" for text in inputs]\n",
    "\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples['highlights'], max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sLFGPx4X7i5R",
    "outputId": "3c9f091d-ead9-4089-f76f-c26552ba2433"
   },
   "outputs": [],
   "source": [
    "# Quick EDA\n",
    "def avg_len(ds, key='article'):\n",
    "    return np.mean([len(x.split()) for x in ds[key]])\n",
    "\n",
    "print('Avg article words (train):', avg_len(train_ds, 'article'))\n",
    "print('Avg summary words (train):', avg_len(train_ds, 'highlights'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170,
     "referenced_widgets": [
      "bf413cd4fdd74cdeac636d6c4a962e2b",
      "8b8ee2ffd6d84b87b1b337ca8706fe6f",
      "36821cf3020543f48bb84210e610d0b7",
      "69650f54ae9f4252a446ee139d2c1230",
      "b0736051a9424d2c84fe0a74e56e8093",
      "bdda59d0818f4fb4910ecdd7b98f52c6",
      "1fa558aab5e84e96b870e1ce5bf3c05c",
      "dcd540f4bce74609b9b716d68c27d546",
      "4a9dc7b899b440e0bf8cf30f5599eefa",
      "a8bde459a324412ca15519fb14d6ea1e",
      "3a78ddc8097c4f468542d4a687b16dfc",
      "7a0c295c56314cc0adbd75d429cd96bd",
      "5e0c8474ae1e4fee8642f7206042f264",
      "3d0e59d5b5d74548993a363454e1462a",
      "8702a60c80954272a861ad75c82457b0",
      "7405f1ce1e8840bbadc785effe0b9924",
      "b5f695c4b7414ab69538e11b510cd453",
      "5293a7e971414c388236371f295cec9b",
      "bd9da12a486e41298bf378068164b676",
      "11d503ce4c06412e98750256cb9ff6b8",
      "92d787186b1f440391b14d1e8cde68c7",
      "ca356d5a78a644aaae0fd7aadd4eefc1"
     ]
    },
    "id": "WZuuYnMZ7mi0",
    "outputId": "098f06ff-4f23-4d43-c280-442775452c37"
   },
   "outputs": [],
   "source": [
    "#Tokenize Datasets\n",
    "print('Tokenizing...')\n",
    "train_tok = train_ds.map(lambda x: preprocess_function(x), batched=True, remove_columns=train_ds.column_names)\n",
    "val_tok = val_ds.map(lambda x: preprocess_function(x), batched=True, remove_columns=val_ds.column_names)\n",
    "print('Tokenization complete. Columns:', train_tok.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wjY2VLHE-RwV"
   },
   "source": [
    "# Model Fine-Tuning and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z7pxiu9h-USL"
   },
   "source": [
    "## Configure Trainer and Metrics\n",
    "We initialize the `t5-small` model and configure the `Seq2SeqTrainer` with specific arguments:\n",
    "* **`eval_strategy='epoch'`**: Evaluation is run at the end of each epoch to track progress. (This is the fixed argument name).\n",
    "* **`fp16=True`**: Uses 16-bit precision for faster training and reduced memory footprint on modern GPUs.\n",
    "* **`report_to='none'`**: Prevents errors by disabling automatic logging to external tools like Weights & Biases.\n",
    "* **ROUGE Metric**: The `compute_metrics` function handles post-processing (decoding tokens and handling padding) and calculates ROUGE scores, which measure the overlap between the generated summary and the reference summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "5c80c24d1f9343a688d10f16ba82bd98",
      "818186e6c9904d2fb28e084911968f83",
      "a7fa5fe053264654936cc1aac471eda8",
      "717fcfb37e4a4a9399fe3741420dfb64",
      "b8229ad9901f4aa38daf613c754d6ec1",
      "41777f29566e444c9be730ec13e36b3f",
      "b24a9c94317a41288de4714d81126777",
      "de7c039e9eda4903bc2d42cd78a36de7",
      "2ae763aa1d9f4445ac4ba7b3b25ef07f",
      "999a1b28415f4ff589b83d70c8fa798f",
      "373d14bc8f8548d7a093e588ca5149e2",
      "673711872b614cf8b756c95ce1aa5a2a",
      "697af0dc60c548baa525093c9d1b919a",
      "6b22517dfd744252b20091701603ce96",
      "04ae609ca6d548d6bf7bd868d243e741",
      "0b9f0371325f4250a177ed9a957f712f",
      "21f7db997a484070a1db12b71265a224",
      "b3990f7d723c4525a152a30b0bf91855",
      "672f6bb4bdcd4788abc8e698eb7e4411",
      "2eb0f1d309cf4c6ca34c178e303a79e4",
      "b23217143f64401f8c3e24b1250f89ed",
      "b98e430d6cd4416ca6ff4ad9de1eeabd",
      "1102e75618d7474abeedefaa3da77409",
      "0bd58f11bf3343b2b4924cdec44ab921",
      "b25ced03ee11496681a49b78b5ede01f",
      "d3ed325523434876aa6d8534c2c96930",
      "2b35dc676afe43af8b54ce6cb52ffb5f",
      "c3978f92398648f391614e2e4a92ae05",
      "8b16a96db3f0433bae8e4a199643a832",
      "9c6f150018ff4e0682d31288087a7c54",
      "823526dc59344d67904c8f6e4af503d0",
      "2b18c3feb87a42aba1c522481da8cfdf",
      "8a317509ee924873972ddbcb29e5883f"
     ]
    },
    "id": "TNY-wu3D7rnX",
    "outputId": "deb45538-9f5a-4958-ff5d-8ed36bc5dd3e"
   },
   "outputs": [],
   "source": [
    "#Model and Trainer Setup\n",
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
    "import evaluate\n",
    "\n",
    "MODEL_NAME = 't5-small'\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jWiBOTMC7xzd"
   },
   "outputs": [],
   "source": [
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121,
     "referenced_widgets": [
      "8e1016c6b1ea4e08b39f68ed4e6d518b",
      "7816bbf4564845f796003c70ab7bfc01",
      "20539ea7d06946669afb963ea17d4d47",
      "f1b3c9ea10ad429699a257e4348cd61a",
      "6698c91af90849a7b2c7aa1be7c2e407",
      "bbfb498ca3614f7bab85d555f2046607",
      "49dea4ffde104e0281a1cecc3c38fb10",
      "bab24b5017644c16b14f4452aba132a0",
      "598439ee47064002b66a99a2e07c13cd",
      "4019dd55d6684743a736ed42da0788f8",
      "cb3e0015cac74e668e4c079725cfe9b6"
     ]
    },
    "id": "OUCwd1gY74JM",
    "outputId": "f47df6d7-ee05-4d64-acf5-9ec52758c711"
   },
   "outputs": [],
   "source": [
    "# Use fp16 only if CUDA available\n",
    "use_fp16 = torch.cuda.is_available()\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=str(PROJECT_DIR / 'results'),\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    predict_with_generate=True,\n",
    "    logging_steps=50,\n",
    "    # üåü FIX 1: Renamed from 'evaluation_strategy' to 'eval_strategy'\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    num_train_epochs=2,\n",
    "    save_total_limit=1,\n",
    "    fp16=use_fp16,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    # üåü FIX 2: Added to prevent wandb/external logging errors\n",
    "    report_to='none',\n",
    ")\n",
    "\n",
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # convert labels to numpy and replace -100\n",
    "    labels = np.array(labels)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    # Format the results as percentages\n",
    "    return {k: round(v * 100, 4) for k, v in result.items()}\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=val_tok,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print('Trainer ready. To start training run the next cell:')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FwFTpdj8-nta"
   },
   "source": [
    "## Execute Fine-Tuning\n",
    "Training the model on the 500-sample subset for 2 epochs. The results of the final evaluation will be printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "id": "yJ6afdTV795X",
    "outputId": "f2cac8da-0815-415b-9ae4-1d47fb40a65b"
   },
   "outputs": [],
   "source": [
    "#Start Training\n",
    "train_result = trainer.train()\n",
    "trainer.save_model(str(PROJECT_DIR / 'saved_t5_small'))\n",
    "print('Training finished. Model saved to', PROJECT_DIR / 'saved_t5_small')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oACQhHZC-34S"
   },
   "source": [
    "# Final Evaluation and Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tagiYxNX-7zA"
   },
   "source": [
    "#### Generate Example Summaries for Review\n",
    "We load the final fine-tuned model and generate summaries for a few validation articles using **Beam Search (`num_beams=4`)**. This step is crucial for visual inspection and demonstrating model quality. The results are saved to `examples.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y5uZTGFD8Ied",
    "outputId": "b8aef451-9f3c-43d3-988c-5d16d03ee487"
   },
   "outputs": [],
   "source": [
    "#Inference and Examples CSV\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# For inference use the saved model if available; otherwise use pretrained\n",
    "try:\n",
    "    # Use the saved model and tokenizer after training\n",
    "    tok_inf = AutoTokenizer.from_pretrained(str(PROJECT_DIR / 'saved_t5_small'))\n",
    "    model_inf = AutoModelForSeq2SeqLM.from_pretrained(str(PROJECT_DIR / 'saved_t5_small'))\n",
    "    print('Loaded saved model for inference.')\n",
    "except Exception:\n",
    "    print('Saved model not found, using pretrained', MODEL_NAME)\n",
    "    tok_inf = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    model_inf = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "examples = val_ds.select(range(8))\n",
    "rows = []\n",
    "for e in examples:\n",
    "    # IMPORTANT: Prepend 'summarize: ' for T5 inference\n",
    "    input_text = f\"summarize: {e['article']}\"\n",
    "\n",
    "    inputs = tok_inf(input_text, return_tensors='pt', truncation=True, max_length=1024)\n",
    "    # Move inputs to GPU if model is on GPU\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.to(model_inf.device) for k, v in inputs.items()}\n",
    "\n",
    "    out = model_inf.generate(**inputs, max_length=130, min_length=30, num_beams=4)\n",
    "    pred = tok_inf.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "    # Only save first 400 chars of the article to keep the CSV clean\n",
    "    rows.append({'article': e['article'][:400] + '...', 'reference': e['highlights'], 'prediction': pred})\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "examples_csv = PROJECT_DIR / 'examples.csv'\n",
    "df.to_csv(str(examples_csv), index=False)\n",
    "print('Saved examples to', examples_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4UZhuUx_C95"
   },
   "source": [
    "## Compute Final ROUGE Score on Validation Set\n",
    "Calculating the final ROUGE score on a larger subset (50 samples) provides the key metric for the project write-up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hNk5ns_K8W4B",
    "outputId": "db51a648-d5b5-410d-93ed-236eb5334c20"
   },
   "outputs": [],
   "source": [
    "#Evaluate ROUGE on 50 validation samples\n",
    "dataset_50 = raw['validation'].select(range(50))\n",
    "print('Computing predictions for 50 samples...')\n",
    "preds = []\n",
    "refs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lGiN7cPk8eXO",
    "outputId": "35d1b7df-0132-40ca-90c5-c7d85b3552e9"
   },
   "outputs": [],
   "source": [
    "# Ensure the model is in eval mode and on the correct device\n",
    "model_inf.eval()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_inf.to(device)\n",
    "\n",
    "for s in dataset_50:\n",
    "    # IMPORTANT: Prepend 'summarize: ' for T5 inference\n",
    "    input_text = f\"summarize: {s['article']}\"\n",
    "\n",
    "    inputs = tok_inf(input_text, return_tensors='pt', truncation=True, max_length=1024)\n",
    "    # Move inputs to device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model_inf.generate(**inputs, max_length=130, min_length=30, num_beams=4)\n",
    "        preds.append(tok_inf.decode(out[0], skip_special_tokens=True))\n",
    "        refs.append(s['highlights'])\n",
    "\n",
    "res = rouge.compute(predictions=preds, references=refs)\n",
    "# Print ROUGE scores in the desired percentage format\n",
    "print('ROUGE results (percent):', {k: round(v*100, 4) for k, v in res.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i_tQeeDS_Nfr"
   },
   "source": [
    "## Prepare Final Output Folder\n",
    "This final step organizes the necessary portfolio files‚Äîthe notebook and the `examples.csv`‚Äîinto a single folder while **EXCLUDING** the large model weights. This is best practice for GitHub submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iiRwTTyN8jvB",
    "outputId": "4d1a26d1-8558-4bb8-fbf8-5a9de0809c64"
   },
   "outputs": [],
   "source": [
    "# CELL 10: Prepare Minimal Final Outputs\n",
    "# ----------------------------------------------------------------------\n",
    "import shutil\n",
    "final_folder = Path('/content/final_outputs')\n",
    "final_folder.mkdir(exist_ok=True)\n",
    "\n",
    "# Define the expected notebook path in Colab\n",
    "notebook_name = 'Project1_Text_Summarizer_COLAB_RUNABLE_FOR_GITHUB.ipynb'\n",
    "notebook_path_in_runtime = '/content/' + notebook_name\n",
    "\n",
    "if Path(notebook_path_in_runtime).exists():\n",
    "    shutil.copy(notebook_path_in_runtime, final_folder)\n",
    "    print('Copied notebook to', final_folder)\n",
    "else:\n",
    "    # Fallback to copy the current running notebook file (if the name is consistent)\n",
    "    try:\n",
    "        # This only works in some Colab environments if you know the exact notebook name\n",
    "        shutil.copy('/content/Project1_Text_Summarizer_COLAB_RUNABLE.ipynb', final_folder)\n",
    "        print('Copied notebook (via fallback name) to', final_folder)\n",
    "    except FileNotFoundError:\n",
    "        print(f'Notebook path not found. Please rename your uploaded file to \"{notebook_name}\" or download the current notebook manually from File->Download .ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1-8jbfAZ8qIt",
    "outputId": "bf458bb8-7c78-42e8-b515-cb35ee5393d2"
   },
   "outputs": [],
   "source": [
    "# Copy examples.csv\n",
    "examples_src = PROJECT_DIR / 'examples.csv'\n",
    "if examples_src.exists():\n",
    "    shutil.copy(str(examples_src), final_folder)\n",
    "    print('Copied examples.csv to', final_folder)\n",
    "else:\n",
    "    print('examples.csv not found; run inference cell first to create it')\n",
    "\n",
    "print('\\n' + '-'*60)\n",
    "print('FINAL STEP: Download the /content/final_outputs folder (right-click in Colab files pane) and upload contents to GitHub.')\n",
    "print('-'*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "knYOqHGXK8wZ"
   },
   "source": [
    "\n",
    "## Model Deployment Preparation (Hugging Face Hub)\n",
    "\n",
    "As the final step, we ensure the fine-tuned model and tokenizer are accessible via the Hugging Face Model Hub. This step validates the entire MLOps workflow and is required to deploy a free, interactive demo using Hugging Face Spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "b694111462a54601b004c8e4f722081e",
      "655ec0e44d354f3e8c731c5c29339d47",
      "3e6956c7957e4ea8b917443f5ee45ff8",
      "e4187e40a8294b478a5c628c2c128978",
      "459217cb3e40469eb7c53c3c141586d6",
      "a04c32f03a0f408d94d8e4508ff38718",
      "30a3aa4b2cd94eb293b9db2f7f2c982f",
      "5a00203c037f4f96ace55e32cec320c7",
      "f0f6f98ddbc94a71b08c467c0c6ffd0b",
      "877c6995dd354970927240985b015b90",
      "3d3fc2aacbd941bd889fe6f77a9bafcf",
      "910f387fe6974ff4b5a094e6d29276ef",
      "f6a7798787804912ae3dd0af7e90a567",
      "2109b5bf1e8c478da3a8ce37669ac8b5",
      "267c0c67c88042c9892e65b898ad73e8",
      "1786f551299b4e359660cc8cdcbfe900",
      "45b5638a195549839ef4edb1c50e1f07",
      "e49d4a77b51545e4bf62833061f28456",
      "df8243d0f9ee4177bedd7aac53c3922a",
      "bc2822d9a0bf4f03b47c344d6616da99"
     ]
    },
    "id": "FBvokEdbLHd1",
    "outputId": "f99d65c8-c731-4c78-c1bc-414b367c8424"
   },
   "outputs": [],
   "source": [
    "#Hugging Face Login\n",
    "!pip install -q huggingface_hub\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "# This will prompt you to enter your Hugging Face token\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 276,
     "referenced_widgets": [
      "52b68a37292a402da39bbb7848abc007",
      "bed0511215ba466483e51bc9abe940d4",
      "339d2a352c0c45bfa1527a29b8d18398",
      "fee88f43ad674607a06456ec56854d7e",
      "f8545c32e3b44fbbb4d9066785f3b8e8",
      "3bcd058202134ee996e3d537f572a722",
      "8f08f0f6e3264b6d80779ddfcbf4d922",
      "407854224de4441f86320552fea6282e",
      "d7eecf26fdff4cc3a0a1935dd83cc17d",
      "15952ea59f0741b58e5fe63fb3d9ea18",
      "e9b4bd770bd34fb7a76994ad63172a29",
      "f8f114c44c3e4927b67523191ce5cbe2",
      "a387a139d9394725aaac2c3dd61603e3",
      "4e7086ed0fff493fa7840deceb3c9abd",
      "cc3fb36312c649c2aa1fe1630db3709f",
      "62380cf37cde4bea9bd37b032f0738ff",
      "22b9ed83ead6407cab587a4b45c8d8e8",
      "6b01f7a49dbb4128860f90394f4e6026",
      "21e4c423a9c948eaae352bd97766d2d6",
      "3de887467f524091b82a03adaeabdaa1",
      "f4e614615dfb497dbdd4c448b28efb02",
      "58c73e82b4a144cbb3f38f14a661a18f",
      "ca97d6224eff450183fdf98262c1a840",
      "fc8a10d040b7429fa174d6665eede5ca",
      "81110cfdfdce48f69ec7d59f6ca913a1",
      "7b440fc8b1b74008bd3e4d99e5373ea7",
      "6634390112054a9c97b89332875ed3b9",
      "3c5d348ff4aa4504b11bcb97e9190a5a",
      "7fff3b02e5f549189f3cde1388079272",
      "3039252e147f4daa885c4814d6a5107b",
      "c90206f62abb4f068587eb0a63607a29",
      "dc3e0a4a89344caf898c0589a230074d",
      "42d42b6b4ffb444cbe8280e4ea0b1810",
      "e14945c6d35247038a2e30457f9c1d83",
      "9c0aea2e5fe34740a247e0f81bc4949c",
      "d397c61ce0e44ab8a65a64918a4d6d6a",
      "d94faa1f9be1434fa919d1b604498f15",
      "47505599ec9b40b288ab73c5779b84bd",
      "4c1fc913371945f9999a81917075a886",
      "aa361eb208c941f3be0dc25f63241425",
      "e712944944d3482daec06d4c9b8b6af3",
      "a77ff4396bb04477911607fcc16db76c",
      "86f4f1b64ee24ea786c1efee9a734b61",
      "845aa3a0a01f4831a30af916a0c83e1d",
      "93feab011ea34648bed2748b6f12804a",
      "e07dd92b2a85450888dcffa27367ec43",
      "2bd02694e9964f7ea0f143c269f9fdc8",
      "8cc640d7d1714b7bb5f362fdde41bf0c",
      "e302efe89287419983cdd4f0c9cea4c0",
      "07bf7b07df2f4abebc0ffa107bc3d9b7",
      "bd89921acc0b495f8394329eb383f69a",
      "e3ab042b523945f39e7d426bf884975e",
      "efbd06fed2a24adeaa63d87e38a2e4d5",
      "eeb302c0621e45598115956a6dcceb50",
      "f68091faaded43f2bb6f6e263d2d9b73",
      "c976b1aba96e46759b72587022af2873",
      "c84782d2bc144e77aa9bb13f7d2c9cbd",
      "8646ca61229c471bb3c7e65018d16a57",
      "c1cd6d46933b4955a391ce5ca759c6d4",
      "ed968574f81e4648a53bb17bd5b2e76a",
      "c00cf8c0e5f9408898bfb4f163c1cb95",
      "85be7bac09b64e1bb9e2203930ebfa57",
      "8f5d4a0efbc040eeb13c4d5a80655aff",
      "e3da33a1e59646859fc9c81741c1cc24",
      "bd608f3495574a098be7886dbda153e2",
      "c002fc5628fc480b8ef189489406796b",
      "88749921593a474ab77bcb34a041187b",
      "5d1f5fe9276d4080b3252851acc1dbc1",
      "b8ce2de9e65a48bf807d9276126cc242",
      "b29ca5add8a843348a3519f0f6f635e0",
      "66b2ef487f3e43e58a2bcdf19b0311f7",
      "7169a9cde2054f739ffcdda0d48da029",
      "c34e35f5e5364c2abfde23ec03e70bd1",
      "80638ef537014380869993b9c2e0e564",
      "9317c9b49bb741c79b8291db6457e9ad",
      "9dced069d1244c16b01249991c47f898",
      "ac46fc49cfe044f6a930b671cbdaa4ba"
     ]
    },
    "id": "01CQRYyrLOg6",
    "outputId": "38e790ae-6c0d-49e1-e024-fa7ccce06964"
   },
   "outputs": [],
   "source": [
    "#Push Model and Tokenizer to the Hub\n",
    "hf_repo_name = \"benjose51/t5-small-cnn-dailymail-summarizer\"\n",
    "\n",
    "# Push the model weights\n",
    "model.push_to_hub(hf_repo_name)\n",
    "\n",
    "# Push the tokenizer config\n",
    "tokenizer.push_to_hub(hf_repo_name)\n",
    "\n",
    "print(f\"‚úÖ Deployment preparation complete.\")\n",
    "print(f\"Your model is now accessible on the Hub at: https://huggingface.co/{hf_repo_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QwCCKIH2_oBH"
   },
   "source": [
    "# üèÜ Project Conclusion & Business Impact\n",
    "\n",
    "The primary objective of this project was to produce high-quality, abstractive text summaries. By successfully fine-tuning a T5-small model on the CNN/DailyMail dataset, we developed a solution that is both highly accurate and computationally efficient, demonstrating an end-to-end MLOps capability.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Performance Metrics (ROUGE Score)\n",
    "\n",
    "The model's performance was evaluated using **ROUGE (Recall-Oriented Understudy for Gisting Evaluation)**, the standard for summarization. The scores below reflect the metrics obtained after **2 epochs** of fine-tuning.\n",
    "\n",
    "| Metric | **Score (Fine-Tuned Model)** | Technical Implication |\n",
    "| :--- | :--- | :--- |\n",
    "| **ROUGE-L** | **$23.2637\\%$** | Measures the longest common subsequence, indicating fluency and content preservation. |\n",
    "| **ROUGE-2** | **$14.2222\\%$** | Measures bigram overlap, indicating phrase structure and better coherence. |\n",
    "| **ROUGE-1** | **$31.5125\\%$** | Measures unigram overlap, indicating the presence of key topics and keywords. |\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Deployment and Business Achievements\n",
    "\n",
    "### 1. Model Deployment\n",
    "The final model is deployed and publicly accessible, validating the project's production readiness:\n",
    "\n",
    "* **Platform:** Hugging Face Spaces (using Gradio)\n",
    "* **Live Demo Link:** [T5-CNN-DailyMail-Summarizer-Demo](https://huggingface.co/spaces/benjose51/T5-CNN-DailyMail-Summarizer-Demo)\n",
    "\n",
    "### 2. Computational Efficiency\n",
    "* **Model Choice:** Selected the **T5-small** model to prioritize fast iteration and cost-effective deployment, maximizing **Return on Investment (ROI)** for high-volume inference.\n",
    "* **Resource Optimization:** Utilized **Mixed Precision Training ($\\text{fp}16$)** for faster training and reduced memory footprint on the GPU.\n",
    "\n",
    "### 3. Technical Mastery\n",
    "* Implemented an end-to-end Hugging Face `transformers` pipeline, including correct setup of the `Seq2SeqTrainer` and integration of the `evaluate` library.\n",
    "* Employed **Beam Search ($\\text{num\\_beams}=4$)** during inference to ensure generated summaries are high-quality, fluent, and avoid repetitive text.\n",
    "\n",
    "---\n",
    "## üßë‚Äçüíª Author and Contact Information\n",
    "\n",
    "This project was developed by:\n",
    "\n",
    "* **Name:** BEN JOSE\n",
    "* **LinkedIn:** [https://www.linkedin.com/in/ben-jose-aa9537190/](https://www.linkedin.com/in/ben-jose-aa9537190/)\n",
    "* **GitHub:** [https://github.com/BENJOSE51](https://github.com/BENJOSE51)\n",
    "* **Email:** benjose51@gmail.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0MoN-gsiSa_x"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
